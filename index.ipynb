{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Gradient Descent Warmup\n", "\n", "The gradients will continue until morale improves"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Theory questions\n", "\n", "#### 1) What is a loss function?\n", "\n", "#### 2) What is the loss function for SSE OLS regression?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\n", "'''\n", "1) A loss function is a function representing the error between the actual values of a target variable\n", "and the predictions made by a model\n", "\n", "A loss function can be used by a machine learning model to minimize error between predicted and target values\n", "\n", "Selection of a loss function often depends on both the specific model being used and the situation in which\n", "one is doing analysis\n", "\n", "2) Loss function for OLS using SSE is mean square error, or loss = sum((y-f(x))**2)/n\n", "\n", "In practice, often calculated as sum((y-f(x))**2)/(2n), so that the exponent is cancelled out when taking the derivative\n", "and the calculations are a little easier.  \n", "'''"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Calculating a loss function for a simple regression model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For this example, we'll import a series of 300 points from the data folder as an `X` feature, and another series of 300 points as a `y` target"]}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [], "source": ["\n", "#data manip\n", "import numpy as np\n", "\n", "#make data\n", "from sklearn.datasets import make_regression\n", "\n", "#testing\n", "from test_scripts.test_class import Test\n", "\n", "#data import\n", "test = Test()\n", "X = test.load_ind('X')\n", "y = test.load_ind('y')\n", "\n", "#used for testing\n", "test = Test()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Calculate the change in the loss function for a simple regression model, $y = \\beta  x + b$, as we step down the gradient once from an initial guess of 3 for $\\beta$ and 1 for $b$\n", "\n", "\n", "#### First, set up\n", "- define n as 300\n", "- define `beta` and `n` as initial guesses of 3 and 1, respectively\n", "- assign the loss function to `loss` using `y`, `X`, `n`, `beta` and `b`\n", "  - note: go ahead and multiply the denominator by 2 to cancel the partial derivative exponent step coming up"]}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [], "source": ["\n", "n = 300\n", "b = 1\n", "beta = 3\n", "\n", "def loss(beta, b, n):\n", "    return sum((y-beta*X-b)**2)/(2*n)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Now, calculate\n", "\n", "- determine the partial derivatives for `beta` and `b`"]}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [], "source": ["\n", "beta_deriv = sum((y-beta*X-b)*-X)/300\n", "b_deriv =sum((y-beta*X-b)*-1)/300"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- re-define `beta` and `b` by \"stepping down the gradient\" - ie, subtract their respective partial derivatives from themselves"]}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [], "source": ["\n", "beta = beta - beta_deriv\n", "b = b - b_deriv\n", "\n", "# #used for testing\n", "# test.save(beta, 'beta')\n", "# test.save(b, 'b')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["- does `y = original_beta * X + original_b` have a smaller loss function value than `y = updated_beta * X + new_b`?"]}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["loss from first guess: 942.4909304580721, loss from updated guess: 15.147998163619848\n"]}], "source": ["\n", "loss_original = loss(3,1,300)\n", "\n", "loss_new = loss(beta, b, 300)\n", "\n", "print(f'loss from first guess: {loss_original}, loss from updated guess: {loss_new}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Now, do this 10 more times!\n", "\n", "- Put the machinery to calculate a new `beta` and `b` you've created inside of a function\n", "  - The function should take as inputs `beta`, `b`, `X`, and `y`\n", "  - (`X` and `y` should have default values)\n", "  - calculate the partial derivative of the loss function for `beta`\n", "  - calculate the partial derivative of the loss function for `b`\n", "  - subtract their respective partial derivatives from `beta` and `b`\n", "  - return updated `beta` and `b`\n", "\n", "\n", "- Put the function inside of a loop which runs 10 times\n", "\n", "- Calculate `beta` and `b`, starting with `beta`=3 and `b`=1, after 10 \"steps down the gradient\"\n", "\n", "- What does it look like the final `y = mx + b` equation should be?"]}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["beta after 10 epochs: 48.34690547882362\n", "b after 10 epochs: 0.017270934505653077\n", "y = 48.34690547882362 * X + 0.017270934505653077, baby\n"]}], "source": ["\n", "def calculate_descent(beta, b, X=X, y=y):\n", "    beta_deriv = sum((y-beta*X-b)*-X)/300\n", "    b_deriv = sum((y-beta*X-b)*-1)/300\n", "    \n", "    new_beta = beta - beta_deriv\n", "    new_b = b - b_deriv\n", "    \n", "    return {'beta':new_beta, 'b': new_b}\n", "\n", "beta = 3\n", "b = 1\n", "\n", "for epoch in range(0,10):\n", "    results = calculate_descent(beta, b)\n", "    beta = results['beta']\n", "    b = results['b']\n", "\n", "print(f'beta after 10 epochs: {beta}')\n", "print(f'b after 10 epochs: {b}')\n", "\n", "print(f'y = {beta} * X + {b}, baby')\n", "\n", "#used for testing\n", "# test.save(beta, 'beta_10')\n", "# test.save(b, 'b_10')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Bonus Round\n", "\n", "Look at the loss function as we move from epoch to epoch\n", "\n", "Why do we not need a learning rate in this instance?\n", "\n", "Why might we need a learning rate in other instances?"]}, {"cell_type": "code", "execution_count": 286, "metadata": {}, "outputs": [], "source": ["\n", "beta = 3\n", "b = 1\n", "\n", "losses = []\n", "for epoch in range(0,10):\n", "    losses.append(loss(beta, b, n))\n", "    results = calculate_descent(beta, b)\n", "    beta = results['beta']\n", "    b = results['b']\n", "    \n", "print(losses)\n", "\n", "'''\n", "This particular loss function is quadratic\n", "\n", "No matter what side of the curve we start on, if we subtract the gradient,\n", "we subtract decreasing values\n", "\n", "We will approach but never reach the global minimum of the function\n", "\n", "But, we might need a learning rate to \"speed up\" or \"slow down\" the \n", "descent down the curve, depending on how sharp the curve is (which\n", "is defined by the data)\n", "\n", "This particular dataset generates a cost curve that steps down nicely, \n", "so we don't need an additional learning rate.\n", "\n", "There are other loss functions that aren't quadratic curves that don't operate this way\n", "\n", "One popular one, a \"hinge\" loss function, decreases linearly up to a specific value and then is 0\n", "\n", "Obviously, subtracting the slope of that cost curve at any given point will not mean we approach but never\n", "attain the global minimum there\n", "\n", "And so we would need a learning rate to use gradient descent for such a loss function\n", "'''"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 4}